{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import scipy\n",
    "from collections import Counter\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as ss\n",
    "from scipy.interpolate import interp1d\n",
    "import hdf5storage\n",
    "import episcanpy.api as epi\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "import anndata\n",
    "import multiprocessing as mp\n",
    "import tempfile\n",
    "from scipy.io import mmwrite\n",
    "import subprocess\n",
    "import rpy2.rinterface as rinterface\n",
    "from sklearn.metrics import adjusted_rand_score,adjusted_mutual_info_score,homogeneity_score,normalized_mutual_info_score\n",
    "import argparse\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义不同的TF-IDF方法\n",
    "def tfidf1(count_mat): \n",
    "    nfreqs = 1.0 * count_mat / np.tile(np.sum(count_mat,axis=0), (count_mat.shape[0],1))\n",
    "    tfidf_mat = np.multiply(nfreqs, np.tile(np.log(1 + 1.0 * count_mat.shape[1] / np.sum(count_mat,axis=1)).reshape(-1,1), (1,count_mat.shape[1])))\n",
    "    return tfidf_mat\n",
    "\n",
    "def tfidf2(count_mat): \n",
    "    tf_mat = 1.0 * count_mat / np.tile(np.sum(count_mat,axis=0), (count_mat.shape[0],1))\n",
    "    signac_mat = np.log(1 + np.multiply(1e4*tf_mat,  np.tile((1.0 * count_mat.shape[1] / np.sum(count_mat,axis=1)).reshape(-1,1), (1,count_mat.shape[1]))))\n",
    "#     return scipy.sparse.csr_matrix(signac_mat)\n",
    "    return signac_mat\n",
    "\n",
    "def tfidf3(count_mat): \n",
    "    model = TfidfTransformer(smooth_idf=False, norm=\"l2\")\n",
    "    model = model.fit(np.transpose(count_mat))\n",
    "    model.idf_ -= 1\n",
    "    tf_idf = np.transpose(model.transform(np.transpose(count_mat)))\n",
    "    return tf_idf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他函数\n",
    "def silhouette(adata, group_key, embed, metric=\"euclidean\", scale=True):\n",
    "    \"\"\"Average silhouette width (ASW)\n",
    "    Wrapper for sklearn silhouette function values range from [-1, 1] with\n",
    "        * 1 indicates distinct, compact clusters\n",
    "        * 0 indicates overlapping clusters\n",
    "        * -1 indicates core-periphery (non-cluster) structure\n",
    "    By default, the score is scaled between 0 and 1 (``scale=True``).\n",
    "    :param group_key: key in adata.obs of cell labels\n",
    "    :param embed: embedding key in adata.obsm, default: 'X_pca'\n",
    "    :param scale: default True, scale between 0 (worst) and 1 (best)\n",
    "    \"\"\"\n",
    "    if embed not in adata.obsm.keys():\n",
    "        print(adata.obsm.keys())\n",
    "        raise KeyError(f\"{embed} not in obsm\")\n",
    "    asw = silhouette_score(\n",
    "        X=adata.obsm[embed], labels=adata.obs[group_key], metric=metric\n",
    "    )\n",
    "    if scale:\n",
    "        asw = (asw + 1) / 2\n",
    "    return asw\n",
    "\n",
    "\n",
    "def check_adata(adata):\n",
    "    if type(adata) is not anndata.AnnData:\n",
    "        raise TypeError(\"Input is not a valid AnnData object\")\n",
    "\n",
    "\n",
    "def check_batch(batch, obs, verbose=False):\n",
    "    if batch not in obs:\n",
    "        raise ValueError(f\"column {batch} is not in obs\")\n",
    "    elif verbose:\n",
    "        print(f\"Object contains {obs[batch].nunique()} batches.\")\n",
    "\n",
    "\n",
    "def recompute_knn(adata, type_):\n",
    "    \"\"\"Recompute neighbours\"\"\"\n",
    "    if type_ == \"embed\":\n",
    "        return sc.pp.neighbors(adata, n_neighbors=15, use_rep=\"X_emb\", copy=True)\n",
    "    elif type_ == \"full\":\n",
    "        if \"X_pca\" not in adata.obsm.keys():\n",
    "            sc.pp.pca(adata, svd_solver=\"arpack\")\n",
    "        return sc.pp.neighbors(adata, n_neighbors=15, copy=True)\n",
    "    else:\n",
    "        # if knn - do not compute a new neighbourhood graph (it exists already)\n",
    "        return adata.copy()\n",
    "\n",
    "\n",
    "def Hbeta(D_row, beta):\n",
    "    \"\"\"\n",
    "    Helper function for simpson index computation\n",
    "    \"\"\"\n",
    "    P = np.exp(-D_row * beta)\n",
    "    sumP = np.nansum(P)\n",
    "    if sumP == 0:\n",
    "        H = 0\n",
    "        P = np.zeros(len(D_row))\n",
    "    else:\n",
    "        H = np.log(sumP) + beta * np.nansum(D_row * P) / sumP\n",
    "        P /= sumP\n",
    "    return H, P\n",
    "\n",
    "\n",
    "def convert_to_one_hot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output 2-D array of one-hot vectors,\n",
    "    where an i'th input value of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "    Example:\n",
    "    .. code-block:: python\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print(one_hot_v)\n",
    "    .. code-block::\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    # assert isinstance(vector, np.ndarray)\n",
    "    # assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector) + 1\n",
    "    # else:\n",
    "    #    assert num_classes > 0\n",
    "    #    assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\n",
    "def compute_simpson_index_graph(\n",
    "    file_prefix=None,\n",
    "    batch_labels=None,\n",
    "    n_batches=None,\n",
    "    n_neighbors=90,\n",
    "    perplexity=30,\n",
    "    chunk_no=0,\n",
    "    tol=1e-5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simpson index of batch labels subset by group.\n",
    "    :param file_prefix: file_path to pre-computed index and distance files\n",
    "    :param batch_labels: a vector of length n_cells with batch info\n",
    "    :param n_batches: number of unique batch labels\n",
    "    :param n_neighbors: number of nearest neighbors\n",
    "    :param perplexity: effective neighborhood size\n",
    "    :param chunk_no: for parallelization, chunk id to evaluate\n",
    "    :param tol: a tolerance for testing effective neighborhood size\n",
    "    :returns: the simpson index for the neighborhood of each cell\n",
    "    \"\"\"\n",
    "    index_file = file_prefix + \"_indices_\" + str(chunk_no) + \".txt\"\n",
    "    distance_file = file_prefix + \"_distances_\" + str(chunk_no) + \".txt\"\n",
    "\n",
    "    # initialize\n",
    "    P = np.zeros(n_neighbors)\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # check if the target file is not empty\n",
    "    if os.stat(index_file).st_size == 0:\n",
    "        print(\"File has no entries. Doing nothing.\")\n",
    "        lists = np.zeros(0)\n",
    "        return lists\n",
    "\n",
    "    # read distances and indices with nan value handling\n",
    "    indices = pd.read_table(index_file, index_col=0, header=None, sep=\",\")\n",
    "    indices = indices.T\n",
    "\n",
    "    distances = pd.read_table(distance_file, index_col=0, header=None, sep=\",\")\n",
    "    distances = distances.T\n",
    "\n",
    "    # get cell ids\n",
    "    chunk_ids = indices.columns.values.astype(\"int\")\n",
    "\n",
    "    # define result vector\n",
    "    simpson = np.zeros(len(chunk_ids))\n",
    "\n",
    "    # loop over all cells in chunk\n",
    "    for i, chunk_id in enumerate(chunk_ids):\n",
    "        # get neighbors and distances\n",
    "        # read line i from indices matrix\n",
    "        get_col = indices[chunk_id]\n",
    "\n",
    "        if get_col.isnull().sum() > 0:\n",
    "            # not enough neighbors\n",
    "            print(f\"Chunk {chunk_id} does not have enough neighbors. Skipping...\")\n",
    "            simpson[i] = 1  # np.nan #set nan for testing\n",
    "            continue\n",
    "\n",
    "        knn_idx = get_col.astype(\"int\") - 1  # get 0-based indexing\n",
    "\n",
    "        # read line i from distances matrix\n",
    "        D_act = distances[chunk_id].values.astype(\"float\")\n",
    "\n",
    "        # start lisi estimation\n",
    "        beta = 1\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "\n",
    "        H, P = Hbeta(D_act, beta)\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "\n",
    "        # first get neighbor probabilities\n",
    "        while np.logical_and(np.abs(Hdiff) > tol, tries < 50):\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta\n",
    "                if betamax == np.inf:\n",
    "                    beta *= 2\n",
    "                else:\n",
    "                    beta = (beta + betamax) / 2\n",
    "            else:\n",
    "                betamax = beta\n",
    "                if betamin == -np.inf:\n",
    "                    beta /= 2\n",
    "                else:\n",
    "                    beta = (beta + betamin) / 2\n",
    "\n",
    "            H, P = Hbeta(D_act, beta)\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        if H == 0:\n",
    "            simpson[i] = -1\n",
    "            continue\n",
    "            # then compute Simpson's Index\n",
    "        batch = batch_labels[knn_idx]\n",
    "        B = convert_to_one_hot(batch, n_batches)\n",
    "        sumP = np.matmul(P, B)  # sum P per batch\n",
    "        simpson[i] = np.dot(sumP, sumP)  # sum squares\n",
    "\n",
    "    return simpson\n",
    "\n",
    "\n",
    "def lisi_graph_py(\n",
    "    adata,\n",
    "    group_key,\n",
    "    n_neighbors=90,\n",
    "    perplexity=None,\n",
    "    subsample=None,\n",
    "    n_cores=1,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to prepare call of compute_simpson_index\n",
    "    Compute LISI score on shortes path based on kNN graph provided in the adata object.\n",
    "    By default, perplexity is chosen as 1/3 * number of nearest neighbours in the knn-graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # use no more than the available cores\n",
    "    n_cores = max(1, min(n_cores, mp.cpu_count()))\n",
    "\n",
    "    if \"neighbors\" not in adata.uns:\n",
    "        raise AttributeError(\n",
    "            \"Key 'neighbors' not found. Please make sure that a kNN graph has been computed\"\n",
    "        )\n",
    "    elif verbose:\n",
    "        print(\"using precomputed kNN graph\")\n",
    "\n",
    "    # get knn index matrix\n",
    "    if verbose:\n",
    "        print(\"Convert nearest neighbor matrix and distances for LISI.\")\n",
    "\n",
    "    batch = adata.obs[group_key].cat.codes.values\n",
    "    n_batches = len(np.unique(adata.obs[group_key]))\n",
    "\n",
    "    if perplexity is None or perplexity >= n_neighbors:\n",
    "        # use LISI default\n",
    "        perplexity = np.floor(n_neighbors / 3)\n",
    "\n",
    "    # setup subsampling\n",
    "    subset = 100  # default, no subsampling\n",
    "    if subsample is not None:\n",
    "        subset = subsample  # do not use subsampling\n",
    "        if isinstance(subsample, int) is False:  # need to set as integer\n",
    "            subset = int(subsample)\n",
    "\n",
    "    # run LISI in python\n",
    "    if verbose:\n",
    "        print(\"Compute knn on shortest paths\")\n",
    "\n",
    "    # set connectivities to 3e-308 if they are lower than 3e-308 (because cpp can't handle double values smaller than that).\n",
    "    connectivities = adata.obsp[\"connectivities\"]  # csr matrix format\n",
    "    large_enough = connectivities.data >= 3e-308\n",
    "    if verbose:\n",
    "        n_too_small = np.sum(large_enough is False)\n",
    "        if n_too_small:\n",
    "            print(\n",
    "                f\"{n_too_small} connectivities are smaller than 3e-308 and will be set to 3e-308\"\n",
    "            )\n",
    "            print(connectivities.data[large_enough is False])\n",
    "    connectivities.data[large_enough is False] = 3e-308\n",
    "\n",
    "    # temporary file\n",
    "    tmpdir = tempfile.TemporaryDirectory(prefix=\"lisi_\")\n",
    "    prefix = tmpdir.name + \"/graph_lisi\"\n",
    "    mtx_file_path = prefix + \"_input.mtx\"\n",
    "\n",
    "    mmwrite(mtx_file_path, connectivities, symmetry=\"general\")\n",
    "    # call knn-graph computation in Cpp\n",
    "\n",
    "    # root = pathlib.Path(scib.__file__).parent  # get current root directory\n",
    "    \n",
    "#     cpp_file_path = (\n",
    "#         root / \"knn_graph/knn_graph.o\"\n",
    "#     )  # create POSIX path to file to execute compiled cpp-code\n",
    "    # comment: POSIX path needs to be converted to string - done below with 'as_posix()'\n",
    "    # create evenly split chunks if n_obs is divisible by n_chunks (doesn't really make sense on 2nd thought)\n",
    "    cpp_file_path = '/home/jianglab/program/scCASdata/integration/scib-main/scib/knn_graph/knn_graph.o'\n",
    "    args_int = [\n",
    "        cpp_file_path, \n",
    "        #cpp_file_path.as_posix(),\n",
    "        mtx_file_path,\n",
    "        prefix,\n",
    "        str(n_neighbors),\n",
    "        str(n_cores),  # number of splits\n",
    "        str(subset),\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(f'call {\" \".join(args_int)}')\n",
    "    try:\n",
    "        subprocess.run(args_int)\n",
    "    except rinterface.embedded.RRuntimeError as ex:\n",
    "        print(f\"Error computing LISI kNN graph {ex}\\nSetting value to np.nan\")\n",
    "        return np.nan\n",
    "\n",
    "    if verbose:\n",
    "        print(\"LISI score estimation\")\n",
    "\n",
    "    if n_cores > 1:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{n_cores} processes started.\")\n",
    "        pool = mp.Pool(processes=n_cores)\n",
    "        chunk_no = np.arange(0, n_cores)\n",
    "\n",
    "        # create argument list for each worker\n",
    "        results = pool.starmap(\n",
    "            compute_simpson_index_graph,\n",
    "            zip(\n",
    "                itertools.repeat(prefix),\n",
    "                itertools.repeat(batch),\n",
    "                itertools.repeat(n_batches),\n",
    "                itertools.repeat(n_neighbors),\n",
    "                itertools.repeat(perplexity),\n",
    "                chunk_no,\n",
    "            ),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        simpson_estimate_batch = np.concatenate(results)\n",
    "\n",
    "    else:\n",
    "        simpson_estimate_batch = compute_simpson_index_graph(\n",
    "            file_prefix=prefix,\n",
    "            batch_labels=batch,\n",
    "            n_batches=n_batches,\n",
    "            perplexity=perplexity,\n",
    "            n_neighbors=n_neighbors,\n",
    "        )\n",
    "\n",
    "    tmpdir.cleanup()\n",
    "\n",
    "    return 1 / simpson_estimate_batch\n",
    "\n",
    "\n",
    "def clisi_graph(\n",
    "    adata,\n",
    "    label_key,\n",
    "    k0=90,\n",
    "    type_=None,\n",
    "    subsample=None,\n",
    "    scale=True,\n",
    "    n_cores=1,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Cell-type LISI (cLISI) score\n",
    "    Local Inverse Simpson’s Index metrics adapted from https://doi.org/10.1038/s41592-019-0619-0 to run on all full\n",
    "    feature, embedding and kNN integration outputs via shortest path-based distance computation on single-cell kNN\n",
    "    graphs.\n",
    "    By default, this function returns a value scaled between 0 and 1 instead of the original LISI range of 0 to the\n",
    "    number of labels.\n",
    "    :param adata: adata object to calculate on\n",
    "    :param group_key: group column name in ``adata.obs``\n",
    "    :param k0: number of nearest neighbors to compute lisi score\n",
    "        Please note that the initial neighborhood size that is\n",
    "        used to compute shortest paths is 15.\n",
    "    :param `type_`: type of data integration, either knn, full or embed\n",
    "    :param subsample: Percentage of observations (integer between 0 and 100)\n",
    "        to which lisi scoring should be subsampled\n",
    "    :param scale: scale output values between 0 and 1 (True/False)\n",
    "    :param n_cores: number of cores (i.e. CPUs or CPU cores to use for multiprocessing)\n",
    "    :return: Median of cLISI scores per cell type labels\n",
    "    \"\"\"\n",
    "\n",
    "    check_adata(adata)\n",
    "    check_batch(label_key, adata.obs)\n",
    "\n",
    "    adata_tmp = recompute_knn(adata, type_)\n",
    "\n",
    "    scores = lisi_graph_py(\n",
    "        adata=adata_tmp,\n",
    "        group_key=label_key,\n",
    "        n_neighbors=k0,\n",
    "        perplexity=None,\n",
    "        subsample=subsample,\n",
    "        n_cores=n_cores,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # cLISI: 1 good, nlabs bad\n",
    "    clisi = np.nanmedian(scores)\n",
    "\n",
    "    if scale:\n",
    "        nlabs = adata.obs[label_key].nunique()\n",
    "        clisi = (nlabs - clisi) / (nlabs - 1)\n",
    "\n",
    "    return clisi\n",
    "\n",
    "\n",
    "def run_louvain(\n",
    "    adata,\n",
    "    label_key,\n",
    "    cluster_key,\n",
    "    range_min=0,\n",
    "    range_max=3,\n",
    "    max_steps=30,\n",
    "    opt_function=\"NMI\",\n",
    "    resolutions=None,\n",
    "    use_rep=None,\n",
    "    inplace=True,\n",
    "    plot=False,\n",
    "    force=True,\n",
    "    verbose=True,\n",
    "    seed=0,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Optimised Louvain clustering\n",
    "    Louvain clustering with resolution optimised against a metric\n",
    "    :param adata: anndata object\n",
    "    :param label_key: name of column in adata.obs containing biological labels to be\n",
    "        optimised against\n",
    "    :param cluster_key: name of column to be added to adata.obs during clustering.\n",
    "        Will be overwritten if exists and ``force=True``\n",
    "    :param resolutions: list of resolutions to be optimised over. If ``resolutions=None``,\n",
    "        default resolutions of 20 values ranging between 0.1 and 2 will be used\n",
    "    :param use_rep: key of embedding to use only if ``adata.uns['neighbors']`` is not\n",
    "        defined, otherwise will be ignored\n",
    "    :returns:\n",
    "        Tuple of ``(res_max, score_max, score_all)`` or\n",
    "        ``(res_max, score_max, score_all, clustering)`` if ``inplace=False``.\n",
    "        ``res_max``: resolution of maximum score;\n",
    "        ``score_max``: maximum score;\n",
    "        ``score_all``: ``pd.DataFrame`` containing all scores at resolutions. Can be used to plot the score profile.\n",
    "        ``clustering``: only if ``inplace=False``, return cluster assignment as ``pd.Series``\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Clustering...\")\n",
    "\n",
    "\n",
    "    if cluster_key in adata.obs.columns:\n",
    "        if force:\n",
    "            print(\n",
    "                f\"Warning: cluster key {cluster_key} already exists \"\n",
    "                \"in adata.obs and will be overwritten\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"cluster key {cluster_key} already exists in \"\n",
    "                + \"adata, please remove the key or choose a different name.\"\n",
    "                + \"If you want to force overwriting the key, specify `force=True`\"\n",
    "            )\n",
    "\n",
    "    if resolutions is None:\n",
    "        n = 20\n",
    "        resolutions = [2 * x / n for x in range(1, n + 1)]\n",
    "\n",
    "    score_max = 0\n",
    "    res_max = resolutions[0]\n",
    "    clustering = None\n",
    "    NMI_score_all = []\n",
    "    ARI_score_all = []\n",
    "    Homo_score_all = []\n",
    "    AMI_score_all = []\n",
    "    \n",
    "    try:\n",
    "        adata.uns[\"neighbors\"]\n",
    "    except KeyError:\n",
    "        if verbose:\n",
    "            print(\"computing neighbours for opt_cluster\")\n",
    "        sc.pp.neighbors(adata, use_rep=use_rep)\n",
    "    # find optimal cluster parameter and metric\n",
    "    n_cluster = np.unique(adata.obs[label_key]).shape[0]\n",
    "    for res in resolutions:\n",
    "        sc.tl.louvain(adata, resolution=res, key_added=cluster_key, random_state=seed)\n",
    "        NMI_score = normalized_mutual_info_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "        ARI_score = adjusted_rand_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "        Homo_score = homogeneity_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "        AMI_score = adjusted_mutual_info_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "        #(adata, label_key, cluster_key, **kwargs)\n",
    "        if verbose:\n",
    "            print(f\"resolution: {res}, NMI: {NMI_score}, ARI: {ARI_score}, Homo: {Homo_score}, AMI: {AMI_score}\")\n",
    "        if opt_function=='NMI':\n",
    "            score = NMI_score\n",
    "        elif opt_function=='ARI':\n",
    "            score = ARI_score\n",
    "        elif opt_function=='Homo':\n",
    "            score = Homo_score\n",
    "        elif opt_function=='AMI':\n",
    "            score = AMI_score\n",
    "#         score_all.append(score)\n",
    "        NMI_score_all.append(NMI_score)\n",
    "        ARI_score_all.append(ARI_score)\n",
    "        Homo_score_all.append(Homo_score)\n",
    "        AMI_score_all.append(AMI_score)\n",
    "        if score_max < score:\n",
    "            score_max = score\n",
    "            [NMI_score_max, ARI_score_max, Homo_score_max, AMI_score_max] = [NMI_score, ARI_score, Homo_score, AMI_score]\n",
    "            res_max = res\n",
    "            clustering = adata.obs[cluster_key]\n",
    "        del adata.obs[cluster_key]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"optimised clustering against {label_key}\")\n",
    "        print(f\"optimal cluster resolution: {res_max}\")\n",
    "        print(f\"selected optimal cluster metrics: {opt_function} {score_max}\")\n",
    "        print(f\"NMI: {NMI_score_max}, ARI: {ARI_score_max}, Homo: {Homo_score_max}, AMI: {AMI_score_max}\")\n",
    "    this_step = 0\n",
    "    this_min = float(range_min)\n",
    "    this_max = float(range_max)\n",
    "    while this_step < max_steps:\n",
    "        this_resolution = this_min + ((this_max-this_min)/2)\n",
    "        sc.tl.louvain(adata,resolution=this_resolution,key_added=cluster_key, random_state=seed)\n",
    "        this_clusters = adata.obs[cluster_key].nunique()\n",
    "        if this_clusters > n_cluster:\n",
    "            this_max = this_resolution\n",
    "        elif this_clusters < n_cluster:\n",
    "            this_min = this_resolution\n",
    "        else:\n",
    "            NMI_score = normalized_mutual_info_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "            ARI_score = adjusted_rand_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "            Homo_score = homogeneity_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "            AMI_score = adjusted_mutual_info_score(adata.obs[label_key], adata.obs[cluster_key])\n",
    "            print(\"louvain clustering with a binary search\")\n",
    "            print(f\"NMI: {NMI_score}, ARI: {ARI_score}, Homo: {Homo_score}, AMI: {AMI_score}\")\n",
    "            resolutions = [this_resolution] + resolutions\n",
    "            NMI_score_all = [NMI_score] + NMI_score_all\n",
    "            ARI_score_all = [ARI_score] + ARI_score_all\n",
    "            Homo_score_all = [Homo_score] + Homo_score_all\n",
    "            AMI_score_all = [AMI_score] + AMI_score_all\n",
    "            break\n",
    "        this_step += 1\n",
    "    resolutions = [res_max] + resolutions\n",
    "    NMI_score_all = [NMI_score_max] + NMI_score_all\n",
    "    ARI_score_all = [ARI_score_max] + ARI_score_all\n",
    "    Homo_score_all = [Homo_score_max] + Homo_score_all\n",
    "    AMI_score_all = [AMI_score_max] + AMI_score_all\n",
    "    score_all = pd.DataFrame(\n",
    "        zip(resolutions, NMI_score_all, ARI_score_all, Homo_score_all, AMI_score_all), columns=(\"resolution\", \"NMI\", \"ARI\", \"Homo\", \"AMI\")\n",
    "    )\n",
    "    if plot:\n",
    "        # score vs. resolution profile\n",
    "        sns.lineplot(data=score_all, x=\"resolution\", y=\"NMI\").set_title(\n",
    "            \"Optimal cluster resolution profile\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    if inplace:\n",
    "        adata.obs[cluster_key] = clustering\n",
    "        return score_all\n",
    "    else:\n",
    "        return score_all, clustering\n",
    "    \n",
    "# @jit()\n",
    "def pearson_sparse(count, nY):\n",
    "    '''\n",
    "    count以np.array的形式存储\n",
    "    '''\n",
    "    Y_mean = nY / count.shape[0]\n",
    "    # mean = []\n",
    "    # var = []\n",
    "    mean = np.zeros(count.shape[1])\n",
    "    var = np.zeros(count.shape[1])\n",
    "    count_new = count - Y_mean\n",
    "    b = np.sum(count_new**2,axis=0)\n",
    "    for i in range(count.shape[1]):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"processing...{i}/{count.shape[1]} {int(i/count.shape[1] * 100)}%\")\n",
    "        # 计算第i个peak与其他所有的相关系数，得到均值和方差\n",
    "        X = count[:,i]\n",
    "        X_new = count_new[:,i]\n",
    "        a = np.dot(X_new,count_new)\n",
    "        corr = a / (np.sqrt(np.multiply(np.dot(X_new, X_new),b)))\n",
    "        \n",
    "        # mean.append(np.mean(corr))\n",
    "        # var.append(np.mean(corr**2))\n",
    "        mean[i] = np.mean(corr)\n",
    "        var[i] = np.mean(corr**2)\n",
    "    print(f\"processing...{count.shape[1]}/{count.shape[1]} {int((count.shape[1])/count.shape[1] * 100)}%\")\n",
    "    # return np.array(mean),np.array(var)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_num = 20000\n",
    "seed_base = 2\n",
    "filter_para = 0.01\n",
    "print('load dataset...')\n",
    "file_path = '../data/'\n",
    "ATAC_count = np.load(file_path + '/atac_count.npz')['arr_0']\n",
    "with open(file_path + '/atac.json','r',encoding='utf8') as fp:\n",
    "    ATAC = json.load(fp)\n",
    "label = ATAC['cell_name']\n",
    "peaks = []\n",
    "for peak in ATAC['peak_name']:\n",
    "    peaks.append(peak[0]+'_'+peak[1]+'_'+peak[2])\n",
    "peaks = np.array(peaks)\n",
    "ATAC_all = sc.AnnData(scipy.sparse.csc_matrix(ATAC_count))\n",
    "ATAC_all.obs['label'] = ATAC['cell_name']\n",
    "ATAC_all.var_names = peaks\n",
    "ATAC_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
